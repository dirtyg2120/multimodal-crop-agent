\chapter{Related Works}
\label{chap:works}
\thispagestyle{open}
\section{The Big Picture of AI in Crop Health Management}

The application of Artificial Intelligence in agriculture has evolved through three distinct paradigms, moving from manual statistical analysis to the current era of generative reasoning. Recent surveys by Liakos et al. \cite{liakos2018machine} and Kamilaris and Prenafeta-Bold√∫ \cite{kamilaris2018deep} classify these techniques based on their architectural complexity and autonomy. While early methods relied on rigid, hand-crafted rules, the field has progressively shifted toward data-driven Deep Learning, and most recently, toward Foundation Models that enable agents to "reason" rather than just classify.

\subsection{Traditional Machine Learning Approaches}
Prior to the widespread adoption of Deep Learning, crop health monitoring was dominated by "feature engineering." In this paradigm, domain experts manually defined the visual characteristics of a disease-such as specific color histograms, texture patterns (e.g., GLCM), or shape descriptors. These features were then processed by classical classifiers like Support Vector Machines (SVM) or Random Forests \cite{liakos2018machine}.

While pioneers like Pydipati et al. \cite{pydipati2006identification} demonstrated success in controlled environments, these systems lacked robustness. They struggled to generalize in real-world conditions where lighting varies, or when symptoms appear subtly different across plant varieties. The rigidity of manual feature extraction meant that the system could only detect what it was explicitly programmed to look for, creating a bottleneck in scalability.

\subsection{Supervised Deep Learning Dominance}
The introduction of Convolutional Neural Networks (CNNs) marked a pivotal shift, allowing systems to automatically learn features from raw data. Early works by Mohanty et al. \cite{mohanty2016using} and Ferentinos \cite{ferentinos2018deep} utilized architectures like AlexNet and GoogLeNet on the PlantVillage dataset, achieving human-level accuracy in disease classification.

This era evolved rapidly from simple classification (labelling an entire image) to real-time object detection, often referred to as the "YOLO Era." Single-stage detectors such as YOLO (You Only Look Once) became the industry standard for field scouting, enabling the localization of specific pests or lesions in milliseconds \cite{li2021comprehensive}. However, these systems operate on a "closed-set" assumption: they can only detect the specific classes they were trained on. If a supervised model encounters a novel pest or an undocumented disease variant, it will either miss it entirely or confidently misclassify it, highlighting a critical limitation in open-field adaptability.

\subsection{The Emergence of Foundation Models and Agentic AI}
The current frontier addresses the limitations of closed-set supervision through \textbf{Foundation Models}. Unlike their predecessors, models like CLIP \cite{radford2021learning} and Grounding DINO \cite{liu2023grounding} are trained on massive, internet-scale datasets of image-text pairs. This enables "Zero-Shot" detection, where a system can identify a pest it has never seen before, simply by understanding its textual description.

Furthermore, the integration of Large Language Models (LLMs) has given rise to \textbf{Agentic AI}. Rather than acting as passive detectors, these systems function as autonomous agents capable of "Active Perception"-observing a symptom, verifying it, and retrieving external knowledge (RAG) to form a diagnosis. This shift moves the goalpost from simple detection accuracy to holistic, actionable decision support.


\section{Problem Definitions}

%3.2.1 The Data Scarcity Problem: High cost of annotation for rare pests.
%
%3.2.2 The Small Object Problem: Difficulty detecting insects in wide-field images.
%
%3.2.3 The Semantic Gap: The disconnect between detecting a "spot" and knowing it is "Early Blight."
\section{Review of Single-Modality Approaches}

%3.3.1 Limitations of Supervised Detectors (YOLOv8, SSD) regarding unseen classes.
%
%3.3.2 Limitations of Text-Only LLMs regarding visual context (Hallucination).
\section{Review of Multi-Modal Approaches}
%3.4.1 Existing Visual Question Answering (VQA) systems in Agronomy.
%
%3.4.2 Recent applications of Foundation Models (DINO/SAM) in agriculture.

\section{Research Gap}
%Synthesis: Lack of systems that integrate Active Perception (SAHI + DINO) with Structured Reasoning (Agentic RAG) into a unified, autonomous pipeline.