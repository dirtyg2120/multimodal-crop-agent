\chapter{Theoretical Background}
\label{chap:theory}
\thispagestyle{open}

\section{Deep Learning in Computer Vision}
\subsection{The Evolution: CNNs to Transformers}
% Since the success of AlexNet in 2012\cite{russakovsky2015ilsvrc}
Since 2012, Convolutional Neural Networks (CNNs) have formed the backbone of most computer vision systems. Successive architectures, including VGG\cite{simonyan2014vgg}, ResNet\cite{he2016resnet}, and EfficientNet\cite{tan2019efficientnet}, consistently achieved strong performance on large-scale benchmarks such as ImageNet\cite{russakovsky2015ilsvrc} and COCO\cite{lin2014coco}. Besides that, CNN-based detectors like YOLO\cite{redmon2016yolo} became widely adopted in real-time applications due to their computational efficiency. CNNs are built to focus on local patterns and recognize them anywhere, which makes them efficient, lightweight, and ideal for running on edge devices.

For tasks requiring global context, this same locality bias is a bottleneck. Capturing long-range dependencies---such as spatial relationships between distant plant organs or complex scene configurations---often requires very deep CNN architectures or additional fine-tuned modules. These designs increase model complexity and may still be insufficient for fine-grained recognition tasks where entire scene understanding is essential.

The introduction of Vision Transformers (ViTs)\cite{dosovitskiy2021vit} marked a fundamental change in computer vision . ViTs model global interactions from the first layer by using self-attention mechanisms and treating images as sequences of patches. Unlike the hierarchical aggregation used in CNNs, the self-attention mechanism allows every patch to attend to every other patch, which is particularly advantageous for capturing long-range dependencies in complex scenes.

There are two primary implications for modern vision systems: \begin{itemize} 
	\item \textbf{Global Reasoning:} ViTs are better able to handle occlusion and complex spatial distributions due to their natural integration of global structure.
	\item \textbf{Multimodal Alignment:} Since Transformers process image patches and text tokens via the same attention mechanism, they make it easier to create unified embedding spaces . This is the foundation of models like CLIP, enabling the zero-shot and open-vocabulary capabilities required for adaptable crop health management in open-set agricultural environments.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The Core Problem: Closed-Set vs. Open-Set Detection}

Conventional object detectors, from two-stage models like Faster R-CNN\cite{fasterrcnn_ren2015} to one-stage architectures like YOLO, typically operate in a closed-set setting. These models learn a mapping from an image to a fixed, finite set of categories $\mathcal{Y}_{\text{known}}$ (e.g., the 80 classes in MS COCO)\cite{lin2014coco}. At test time, the model implicitly assumes that all visible objects belong to this known set. Any object outside this taxonomy—such as a rare pest or a novel disease symptom—is either forced into a known class (misclassification) or treated as background (suppression).

\emph{Open-set object detection} (OSOD) generalizes this formulation. It requires a model to (i) accurately classify instances from $\mathcal{Y}_{\text{known}}$, while (ii) acknowledging inputs from $\mathcal{Y}_{\text{unknown}}$ without overconfident misclassification.\cite{dhamija2020openset} While early OSOD work focused on rejecting unknowns, recent advances in \emph{open-vocabulary} models provide a more flexible solution. By leveraging large-scale vision--language pre-training (e.g., CLIP), these models replace fixed classification layers with text-embedding similarity scores.\cite{radford2021clip,zhu2024surveyovd} This allows a detector not just to reject an unknown object, but to localize it based on descriptive text prompts (e.g.,``necrotic lesion with yellow halo'') that were never seen during training.

This distinction is critical because agriculture is inherently an \emph{open-set} environment. The biological landscape is dynamic: new pathogens emerge, invasive species migrate, and symptom appearances shift due to environmental stress.\cite{fuentes2021opensettomato} A closed-set detector is brittle in this context; it cannot recognize a threat it has not been explicitly trained to see. Recent work in plant pathology underlines the necessity of models that can handle these ``unknown'' classes to prevent silent failures in the field.\cite{shen2024opensetpest}

In this thesis, we address this limitation by operationalizing crop health monitoring as a \emph{text-conditioned, open-set} problem. Rather than relying on a static list of pest categories, we propose a pipeline that integrates open-vocabulary detection with agronomic knowledge. The system is designed to localize both known and novel visual patterns by conditioning the detector on textual descriptions, thereby bridging the gap between rigid closed-set training and the dynamic reality of agricultural management.

%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Grounding DINO}
Grounding DINO is an object detection model capable of detecting any object. By inputting the object you want to detect in text, you can obtain the bounding box for that object within an image.
\subsection{Detection Transformers (DETR)}

\subsection{DINO}

\subsection{GLIP and Grounded Pre-Training}

\subsection{Grounding DINO Architecture}

%Grounding-DINO is a popular large vision-language model equipped with open-set object detection capabilities. This enables the recognition of novel objects outside the initially defined categories.
%As shown in Figure 2, the Grounding-DINO model processes an image along with a text prompt corresponding to each class. The model comprises several key components:
%an image backbone that extracts image features, a text backbone that encodes textual information into feature vectors, a feature enhancer module that fuses these image and text features, a language-guided query selection module that initializes object queries based on the input text prompt, and finally, a cross-modality decoder that refines object features and bounding boxes.
%For each (image, text) pair, the model processes images using the Swin Transformer[35] and text via a BERT encoder [13]. Image features are hierarchically extracted from different layers of the Swin Transformer, capturing hierarchical visual information. Text is tokenized using the byte-pair encoding (BPE) scheme [49], which is then encoded by the BERT model to produce N ×768-dimensional text embeddings (features), where N represents the number of tokens in the text prompt. These raw image and text embeddings are fed into the feature enhancer module to enable cross-modal fusion. The module comprises multiple enhancer layers, each containing self-attention mechanisms for both image and text processing, as well as cross attention layers that facilitate interactions between text-to-image and image-to-text contexts.
%Grounding-DINO method uses the enhanced text features to select object queries by calculating the dot product between text and image features. It then selects NI object queries by choosing image features with the maximum scores. These language-guided queries are subse- quently fed into a cross-modality decoder. In this decoder, query features undergo a series of self-attention operations, followed by cross-attentions with both image and text features. Finally, the output queries from the decoder’s last layer are utilized to predict object boxes and corresponding class probabilities based on similarity with the text features. For training, the model uses a contrastive loss between predicted objects and language token features. To compute this loss, each query calculates a dot product with text features to produce logits for every text token. Then the focal loss [31] is applied to these logits to obtain the classification loss. Additionally, L1 and GIoU[45] losses are employed for the bounding box regression. Similar to DETR-like models [7], bipartite matching is used to find the matching between predicted and ground-truth objects. The total loss is then computed using the combination of classification and bounding-box losses based on this mapping.

%%%%%%%%%%%%%%%%%%%%%%%%%

\section{CLIP}

\subsection{Learning from Unstructured Data}

\subsection{Dual Encoders}
- Image Encoder
- Text Encoder
\subsection{Contrastive Learning (The Core)}

\subsection{Zero-Shot Inference}

\subsection{Visual Verification}