# -*- coding: utf-8 -*-
"""Grounding DINO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1brlo0i37_76_RAfPSiJDWb-PCXXthEX3

# Demo

## Setup
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/IDEA-Research/GroundingDINO.git
# %cd GroundingDINO/
!pip install -e .
!mkdir -p weights data
# %cd weights
!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth
# %cd ..

"""## Utils"""

import io
import glob
import argparse
from functools import partial
import cv2
import requests

from io import BytesIO
from PIL import Image
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings("ignore")




from groundingdino.models import build_model
from groundingdino.util.slconfig import SLConfig
from groundingdino.util.utils import clean_state_dict
from groundingdino.util.inference import annotate, load_image, predict, load_model
import groundingdino.datasets.transforms as T

from huggingface_hub import hf_hub_download
import os
import supervision as sv

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

import torch
from torchvision.ops import box_convert

def download_image(url, image_file_path):
    r = requests.get(url, timeout=4.0)
    if r.status_code != requests.codes.ok:
        assert False, 'Status code error: {}.'.format(r.status_code)

    with Image.open(io.BytesIO(r.content)) as im:
        im.save(image_file_path)

    print('Image downloaded from url: {} and saved to: {}.'.format(url, image_file_path))

def generate_masks_with_grounding(image_source, boxes):
    h, w, _ = image_source.shape
    boxes_unnorm = boxes * torch.Tensor([w, h, w, h])
    boxes_xyxy = box_convert(boxes=boxes_unnorm, in_fmt="cxcywh", out_fmt="xyxy").numpy()
    mask = np.zeros_like(image_source)
    for box in boxes_xyxy:
        x0, y0, x1, y1 = box
        mask[int(y0):int(y1), int(x0):int(x1), :] = 255
    return mask


def display_image(image_source, mask, annotated_frame):
    col = 3 if mask is not None else 2
    fig, axes = plt.subplots(1, col, figsize=(12,12))
    imgs = {
        "image_source": image_source,
        "mask": mask,
        "annotated_frame": annotated_frame
    }
    i = 0

    # Plot the images in each subplot
    for k, v in imgs.items():
        if v is not None:
            axes[i].imshow(v)
            axes[i].set_title(k)
            axes[i].axis('off')
            i += 1

    # Show the plots
    plt.tight_layout()
    plt.show()

def load_image_with_url_handling(image_path_or_url):
    transform = T.Compose(
        [
            T.RandomResize([800], max_size=1333),
            T.ToTensor(),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )

    if image_path_or_url.startswith("http://") or image_path_or_url.startswith("https://"):
        response = requests.get(image_path_or_url)
        image_source = Image.open(io.BytesIO(response.content)).convert("RGB")
    else:
        image_source = Image.open(image_path_or_url).convert("RGB")

    image = np.asarray(image_source)
    image_transformed, _ = transform(image_source, None)
    return image_source, image_transformed

"""## Example code"""

gdino_model = load_model("groundingdino/config/GroundingDINO_SwinT_OGC.py", "weights/groundingdino_swint_ogc.pth")

display_image(image_source, None, annotated_frame[...,::-1])

# def load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu'):
#     cache_config_file = hf_hub_download(repo_id=repo_id, filename=ckpt_config_filename)

#     args = SLConfig.fromfile(cache_config_file)
#     model = build_model(args)
#     args.device = device

#     cache_file = hf_hub_download(repo_id=repo_id, filename=filename)
#     checkpoint = torch.load(cache_file, map_location='cpu')
#     log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)
#     print("Model loaded from {} \n => {}".format(cache_file, log))
#     _ = model.eval()
#     return model

# ckpt_repo_id = "ShilongLiu/GroundingDINO"
# ckpt_filenmae = "groundingdino_swint_ogc.pth"
# ckpt_config_filename = "GroundingDINO_SwinT_OGC.cfg.py"

# model = load_model("groundingdino/config/GroundingDINO_SwinT_OGC.py", "weights/groundingdino_swint_ogc.pth")
# model = load_model_hf(ckpt_repo_id, ckpt_filenmae, ckpt_config_filename)

"""### Run Grounding DINO"""

TEXT_PROMPT = "leaf ."
BOX_THRESHOLD = 0.3
TEXT_TRESHOLD = 0.25

image_source, image = load_image("./data/tomato.png")

boxes, logits, phrases = predict(
    model=gdino_model,
    image=image,
    caption=TEXT_PROMPT,
    box_threshold=BOX_THRESHOLD,
    text_threshold=TEXT_TRESHOLD,
    device="cpu"
)

annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)
image_mask = generate_masks_with_grounding(image_source, boxes)

display_image(image_source, image_mask, annotated_frame[...,::-1])

sv.plot_image(annotated_frame)

import numpy as np
import cv2

def extract_crops(image_source, boxes, phrases):
    """
    Extracts crops from the image based on Grounding DINO boxes.
    Returns a list of dictionaries containing the crop and metadata.
    """
    h_img, w_img, _ = image_source.shape
    crops = []

    # Boxes are typically on CPU from your snippet, but ensure they are tensors/arrays
    # Grounding DINO boxes are normalized (0-1) in format [cx, cy, w, h]
    for box, phrase in zip(boxes, phrases):

        # 1. Un-normalize and convert to (x, y, w, h) in pixels
        cx, cy, w, h = box.tolist()

        # 2. Convert Center (cx, cy) to Top-Left corner (x1, y1)
        x1 = int((cx - 0.5 * w) * w_img)
        y1 = int((cy - 0.5 * h) * h_img)
        x2 = int((cx + 0.5 * w) * w_img)
        y2 = int((cy + 0.5 * h) * h_img)

        # 3. Safety Clip (ensure coordinates are within image bounds)
        x1 = max(0, x1)
        y1 = max(0, y1)
        x2 = min(w_img, x2)
        y2 = min(h_img, y2)

        # 4. Perform the Crop (Numpy Slicing: [y:y+h, x:x+w])
        crop_img = image_source[y1:y2, x1:x2]

        # 5. Store crop with its label (useful for your CLIP routing)
        crops.append({
            "label": phrase,  # e.g., "brown leaf"
            "crop": crop_img, # RGB Numpy Array
            "bbox": (x1, y1, x2, y2)
        })

    return crops

# --- Usage in your script ---
cropped_objects = extract_crops(image_source, boxes, phrases)

# Example: Save them or pass to CLIP
for i, obj in enumerate(cropped_objects):
    # Save to disk to verify
    # Note: image_source is usually RGB, OpenCV expects BGR
    cv2.imwrite(f"crop_{i}_{obj['label']}.jpg", cv2.cvtColor(obj['crop'], cv2.COLOR_RGB2BGR))

    print(f"Saved {obj['label']} at {obj['bbox']}")

"""## CLIP"""

!pip install transformers==4.49.0

import torch
from transformers import CLIPModel, CLIPProcessor
from PIL import Image
import requests
from pprint import pprint

# 42 labels in index order – must match training exactly
LABEL_TEXTS = [
    "Apple leaf with Apple scab",                                # 0
    "Apple leaf with Black rot",                                 # 1
    "Apple leaf with Cedar apple rust",                          # 2
    "Healthy Apple leaf",                                        # 3
    "Corn leaf with Cercospora leaf spot (Gray leaf spot)",      # 4
    "Corn leaf with Common rust",                                # 5
    "Corn leaf with Northern Leaf Blight",                       # 6
    "Healthy Corn leaf",                                         # 7
    "Durian leaf with Algal Leaf Spot",                          # 8
    "Durian leaf with Leaf Blight",                              # 9
    "Durian leaf with Leaf Spot",                                # 10
    "Healthy Durian leaf",                                       # 11
    "Grape leaf with Black rot",                                 # 12
    "Grape leaf with Esca (Black Measles)",                      # 13
    "Grape leaf with Leaf blight (Isariopsis Leaf Spot)",        # 14
    "Healthy Grape leaf",                                        # 15
    "Oil Palm leaf with brown spots",                            # 16
    "Healthy Oil Palm leaf",                                     # 17
    "Oil Palm leaf with white scale",                            # 18
    "Orange leaf with Huanglongbing (Citrus greening)",          # 19
    "Pepper bell leaf with Bacterial spot",                      # 20
    "Healthy Pepper bell leaf",                                  # 21
    "Potato leaf with Early blight",                             # 22
    "Potato leaf with Late blight",                              # 23
    "Healthy Potato leaf",                                       # 24
    "Rice leaf with Bacterial blight",                           # 25
    "Rice leaf with Blast",                                      # 26
    "Rice leaf with Brown spot",                                 # 27
    "Rice leaf with Tungro",                                     # 28
    "Healthy Soybean leaf",                                      # 29
    "Strawberry leaf with Leaf scorch",                          # 30
    "Healthy Strawberry leaf",                                   # 31
    "Tomato leaf with Bacterial spot",                           # 32
    "Tomato leaf with Early blight",                             # 33
    "Tomato leaf with Late blight",                              # 34
    "Tomato leaf with Leaf Mold",                                # 35
    "Tomato leaf with Septoria leaf spot",                       # 36
    "Tomato leaf with Spider mites (Two-spotted spider mite)",   # 37
    "Tomato leaf with Target Spot",                              # 38
    "Tomato leaf with Tomato Yellow Leaf Curl Virus",            # 39
    "Tomato leaf with Tomato mosaic virus",                      # 40
    "Healthy Tomato leaf",                                       # 41
]

# Healthy vs diseased indices (auto from text so it’s less error-prone)
HEALTHY_INDICES = [i for i, t in enumerate(LABEL_TEXTS) if "Healthy" in t]
DISEASED_INDICES = [i for i in range(len(LABEL_TEXTS)) if i not in HEALTHY_INDICES]

# Optional: crop groups (not strictly needed, but handy for debugging / future logic)
CROP_GROUPS = {
    "Apple":      [0, 1, 2, 3],
    "Corn":       [4, 5, 6, 7],
    "Durian":     [8, 9, 10, 11],
    "Grape":      [12, 13, 14, 15],
    "Oil Palm":   [16, 17, 18],
    "Orange":     [19],
    "Pepper":     [20, 21],
    "Potato":     [22, 23, 24],
    "Rice":       [25, 26, 27, 28],
    "Soybean":    [29],
    "Strawberry": [30, 31],
    "Tomato":     [32, 33, 34, 35, 36, 37, 38, 39, 40, 41],
}

def load_plant_disease_clip(device: str = "cpu"):
    token = "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"
    model_name = "Keetawan/clip-vit-large-patch14-plant-disease-finetuned"
    model = CLIPModel.from_pretrained(model_name, token=token)
    processor = CLIPProcessor.from_pretrained(model_name, token=token)
    model.to(device)
    model.eval()
    return model, processor

clip_model, processor = load_plant_disease_clip("cpu")

@torch.no_grad()
def classify_disease_with_open_set_logic(
    image,
    model,
    processor,
    device: str = "cpu",
    # ---- thresholds (initial suggestions – tune on your validation set) ----
    min_group_conf: float = 0.60,    # min prob to trust "healthy vs diseased"
    min_disease_top1: float = 0.55,  # min prob for top disease inside diseased group
    min_disease_margin: float = 0.10 # margin (top1 - top2) within diseased group
):
    """
    image: PIL Image or numpy array compatible with CLIPProcessor.
    model, processor: from load_plant_disease_clip().

    Returns a dict:
      {
        "status": "healthy" | "known_disease" | "diseased_unspecified" | "uncertain",
        "label": str | None,      # one of LABEL_TEXTS or None
        "confidence": float,      # overall confidence score in [0, 1]
        "extra": {...}            # various debug/probing signals
      }
    """
    # 1) Prepare inputs
    inputs = processor(
        text=LABEL_TEXTS,
        images=image,
        return_tensors="pt",
        padding=True
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # 2) Forward through CLIP
    outputs = model(**inputs)
    logits = outputs.logits_per_image  # [1, 42]
    logits = logits.squeeze(0)         # [42]

    # Full softmax for reference
    probs_all = logits.softmax(dim=0)  # [42]
    top_prob_all, top_idx_all = probs_all.max(dim=0)
    top_label_all = LABEL_TEXTS[top_idx_all]

    # 3) Group-level: healthy vs diseased (log-sum-exp for stability)
    healthy_logits = logits[HEALTHY_INDICES]
    diseased_logits = logits[DISEASED_INDICES]

    healthy_score = healthy_logits.logsumexp(dim=0)
    diseased_score = diseased_logits.logsumexp(dim=0)

    group_scores = torch.stack([healthy_score, diseased_score])  # [2]
    group_probs = group_scores.softmax(dim=0)
    p_healthy = group_probs[0].item()
    p_diseased = group_probs[1].item()

    # 4) Intra-group details for diseased
    disease_probs = diseased_logits.softmax(dim=0)  # [num_diseased]
    disease_top_prob, disease_top_rel_idx = disease_probs.max(dim=0)
    disease_top_prob = disease_top_prob.item()
    disease_top_global_idx = DISEASED_INDICES[disease_top_rel_idx]
    disease_top_label = LABEL_TEXTS[disease_top_global_idx]

    # Top-2 margin inside diseased group
    if disease_probs.numel() > 1:
        sorted_probs, _ = torch.sort(disease_probs, descending=True)
        disease_second_prob = sorted_probs[1].item()
    else:
        disease_second_prob = 0.0
    disease_margin = disease_top_prob - disease_second_prob

    # 5) Intra-group details for healthy (for debugging / optional use)
    healthy_probs = healthy_logits.softmax(dim=0)
    healthy_top_prob, healthy_top_rel_idx = healthy_probs.max(dim=0)
    healthy_top_prob = healthy_top_prob.item()
    healthy_top_global_idx = HEALTHY_INDICES[healthy_top_rel_idx]
    healthy_top_label = LABEL_TEXTS[healthy_top_global_idx]

    # -------------------------
    # 6) Decision rules
    # -------------------------
    # Default
    status = "uncertain"
    label = None
    confidence = float(max(p_healthy, p_diseased))

    # Case A: model not clearly sure if healthy or diseased
    if max(p_healthy, p_diseased) < min_group_conf:
        status = "uncertain"
        label = None

    # Case B: more likely healthy
    elif p_healthy >= p_diseased:
        # We treat this as healthy – we *can* return the best healthy label
        status = "healthy"
        label = healthy_top_label
        # Confidence combines group-level & per-label confidence
        confidence = float(p_healthy * healthy_top_prob)

    # Case C: more likely diseased
    else:
        # Very confident about a specific known disease
        if (disease_top_prob >= min_disease_top1) and (disease_margin >= min_disease_margin):
            status = "known_disease"
            label = disease_top_label
            confidence = float(p_diseased * disease_top_prob)
        else:
            # This is your open-set "disease but not sure which"
            status = "diseased_unspecified"
            label = None
            # still we know it's probably diseased
            confidence = float(p_diseased)

    # 7) Build an extra debug payload – useful for logging / tuning
    extra = {
        # global view
        "top_label_all": top_label_all,
        "top_prob_all": float(top_prob_all),

        # group-level
        "p_healthy": p_healthy,
        "p_diseased": p_diseased,

        # healthy subgroup details
        "healthy_top_label": healthy_top_label,
        "healthy_top_prob": healthy_top_prob,

        # diseased subgroup details
        "disease_top_label": disease_top_label,
        "disease_top_prob": disease_top_prob,
        "disease_margin": disease_margin,
        "disease_second_prob": disease_second_prob,

        # for future analysis you might log these as histograms
        "raw_probs_all": probs_all.detach().cpu().tolist(),
    }

    return {
        "status": status,
        "label": label,
        "confidence": confidence,
        "extra": extra,
    }

# 2) Load an image
url = "https://www.rainbowgardens.biz/wp-content/uploads/2021/04/IMG-6377.jpg"
# image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
image = Image.open("./test2.png").convert("RGB")

# 3) Classify with open-set logic
result = classify_disease_with_open_set_logic(
    image=image,
    model=clip_model,
    processor=processor,
    device="cpu",
)

display(image)
pprint({k: v for k, v in result.items() if k != 'extra'})

